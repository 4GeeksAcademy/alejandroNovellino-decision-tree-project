{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Get the data"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Get the data processed in the previous notebook (Exploratory Data Analysis)"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt \n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from src.constants import X_TRAIN_PATH, X_TEST_PATH, Y_TRAIN_PATH, Y_TEST_PATH\n",
    "\n",
    "# save the processed data to their corresponding files\n",
    "X_train = pd.read_csv(filepath_or_buffer=X_TRAIN_PATH, sep=',')\n",
    "X_test = pd.read_csv(filepath_or_buffer=X_TEST_PATH, sep=',')\n",
    "\n",
    "y_train = pd.read_csv(filepath_or_buffer=Y_TRAIN_PATH, sep=',')\n",
    "y_test = pd.read_csv(filepath_or_buffer=Y_TEST_PATH, sep=',')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# verify x_train\n",
    "X_train.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# verify x_test\n",
    "X_test.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# verify y_train\n",
    "y_train.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# verify y_test\n",
    "y_test.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Option 1. Model without any optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Initialization and training of the model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# model with max iteration 1000, without 1000 the model give the error ConvergenceWarning\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, y_train.values.ravel())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Model prediction"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# make the prediction\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# make the prediction of the probabilities of being one class or another\n",
    "y_prob = model.predict_proba(X_test)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Let's print the full report of the model"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
    "                            f1_score, roc_auc_score, confusion_matrix,\n",
    "                            log_loss, classification_report)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "auc_roc = roc_auc_score(y_test, y_prob[:, 1])\n",
    "confusion = confusion_matrix(y_test, y_pred)\n",
    "log_loss = log_loss(y_test, y_prob)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "print(f'F1-Score: {f1}')\n",
    "print(f'AUC-ROC: {auc_roc}')\n",
    "print(f'Confusion Matrix:\\n{confusion}')\n",
    "print(f'Log Loss: {log_loss}')\n",
    "print(f'Classification Report:\\n{report}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Here the analysis."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Now let's draw the confusion matrix"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from src.utils import draw_confusion_matrix\n",
    "\n",
    "draw_confusion_matrix(confusion=confusion)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The interpretation of a confusion matrix is as follows:\n",
    "\n",
    "- **True positive (TP)**: corresponds to the number TP and are the cases where the model predicted positive **(what the label means)** and the actual class is also positive.\n",
    "- **True negative (TN)**: Corresponds to the number TN and are the cases where the model predicted negative **(what the label means)** and the actual class is also negative.\n",
    "- **False positive (FP)**: Corresponds to the number FP and are the cases in which the model predicted positive, but the actual class is negative.\n",
    "- **False negative (FN)**: Corresponds to the number FN and are the cases where the model predicted negative, but the actual class is positive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Option 2. Model with optimization"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# create another model\n",
    "opt_model = LogisticRegression(random_state=42)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 1: Create the hyperparameter optimization model"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# define the parameters that we want to adjust by hand, depends on the model to use\n",
    "hyperparams = {\n",
    "\n",
    "}\n",
    "\n",
    "# initialize the grid\n",
    "grid = GridSearchCV(opt_model, hyperparams, scoring = 'accuracy', cv = 5, n_jobs=-1, verbose=1)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "grid.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "print(f\"Best hyperparameters: {grid.best_params_}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Get the best model and predict"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# get the best parameters and model\n",
    "best_params = grid.best_params_\n",
    "best_model: LogisticRegression = grid.best_estimator_"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# predict the values\n",
    "y_pred = best_model.predict(X_test)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# make the prediction of the probabilities of being one class or another\n",
    "y_prob = best_model.predict_proba(X_test)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Metrics of the model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
    "                            f1_score, roc_auc_score, confusion_matrix,\n",
    "                            log_loss, classification_report)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "auc_roc = roc_auc_score(y_test, y_prob[:, 1])\n",
    "confusion = confusion_matrix(y_test, y_pred)\n",
    "log_loss = log_loss(y_test, y_prob)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "print(f'F1-Score: {f1}')\n",
    "print(f'AUC-ROC: {auc_roc}')\n",
    "print(f'Confusion Matrix:\\n{confusion}')\n",
    "print(f'Log Loss: {log_loss}')\n",
    "print(f'Classification Report:\\n{report}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "draw_confusion_matrix(confusion=confusion)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The interpretation of a confusion matrix is as follows:\n",
    "\n",
    "- **True positive (TP)**: corresponds to the number TP and are the cases where the model predicted positive **(what the label means)** and the actual class is also positive.\n",
    "- **True negative (TN)**: Corresponds to the number TN and are the cases where the model predicted negative **(what the label means)** and the actual class is also negative.\n",
    "- **False positive (FP)**: Corresponds to the number FP and are the cases in which the model predicted positive, but the actual class is negative.\n",
    "- **False negative (FN)**: Corresponds to the number FN and are the cases where the model predicted negative, but the actual class is positive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Analysis here."
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Conclusion"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Make a table with comparisons here"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
